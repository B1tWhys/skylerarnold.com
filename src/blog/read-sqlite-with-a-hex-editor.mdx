---
title: "Reading SQLite with just a hex editor"
description: "Deep dive into SQLite internals to learn how it works under the hood"
date: 2025-01-02
---

I recently wrote a sqlite file parser in python from scratch. I thought I'd write a blog post about
the things I learned along the way. I've chosen to keep things as simple as possible, as the goal isn't 
to write a production ready sqlite library (the stdlib already  has one), rather it's to gain a more
concrete understanding of how b-trees are implemented in real world databases.

## Creating a test database
Keeping the goal in mind, I decided to stick with a fairly database schema:

```sql
CREATE TABLE users (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username TEXT NOT NULL UNIQUE,
    email TEXT NOT NULL UNIQUE,
    password_hash TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

It's a pretty basic `users` table. Of note, however, is the `INTEGER PRIMARY KEY AUTOINCREMENT` column, and the 
two `UNIQUE columns` (the username & email). As we'll see, these two unassuming tweaks to the schema have some
pretty significant ramifications on performance for certain queries.

I also needed to bootstrap some data. Here's a script from chatgpt that handles creating the database and filling
in 1k test rows:
```python
import sqlite3

# Connect to SQLite database (or create it if it doesn't exist)
conn = sqlite3.connect("example.db")
cursor = conn.cursor()

# Create the users table if it doesn't exist
cursor.execute("""
CREATE TABLE IF NOT EXISTS users (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username TEXT NOT NULL UNIQUE,
    email TEXT NOT NULL UNIQUE,
    password_hash TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
)
""")

# Insert 1000 test users
users_to_insert = []
for i in range(1, 1001):
    username = f"user_{i}"
    email = f"user_{i}@example.com"
    password = f"password_{i}"  # Predictable test value for password
    users_to_insert.append((username, email, password))

# Use executemany for batch insertion
cursor.executemany("""
INSERT INTO users (username, email, password_hash)
VALUES (?, ?, ?)
""", users_to_insert)

# Commit changes and close connection
conn.commit()
conn.close()
```

The first challenge is reading a user based on the primary key. Basically the equivalent of:
```sql
SELECT * FROM users WHERE id = 450
```

## Overview
Before jumping into writing code, I think it'll make sense to have a high level understanding
of the process. The basic The basic steps to reading a record from a table are:

1. Parse the file header to figure out the page size
2. Read the `sqlite_schema` table to figure out what page the `users` starts on
3. (optional) If we're using an index rather than querying on the `rowid`, we'll use 
the index to look up the `rowid` for the record we need
4. Read the `users` table to find the record corresponding to our target `rowid`

Steps 2 and 4 are basically the same, as they're both just searching a table for a value.

### B-trees
I think the easiest way to learn how a b-tree works is with an example. Here's a (conceptual) diagram
of how our users table is stored:


Lets try to find user 5. First we go to the "root" of the tree. Each page in a sqlite database is
composed of cells. The cells are always sorted in ascending order based on their keys, which
allows us to do a binary search to efficiently look up cells based on their keys. For an interior
table page like this one, cells each contain a key and a pointer to another page. All the keys in the
child page (the one that's pointed to) are guaranteed to be less than or equal to the key in the cell.

So in our example, the smallest key that's \>= our target of 5 is 5, and it's got a pointer to page
8. This tells us that if we're looking for a row in the table with a rowid of 5, it'll either be in
page 8, or one of page 8's children (if page 8 turns out to to be another interior page).

In our case, page 8 is a leaf table page, so its cells contain the actual data for the table,
rather than just pointers to other pages. Once again, the cells are in order based on their key
so we can do another binary search to quickly find cell 5.

There's one last step necessary to parse the actual columns out of the cell's payload, but we'll get
to those implementation details a little later on.


## The File Header

Sqlite files (like so many file formats) start with a header. It's just a bunch of
**big endian** (very important!) fields, concatenated together. Here's a python class I
wrote that handles parsing the database header out of a file. ~~Here's a handy table
for reference, and a decoder. You can paste the first 100 (or more) bytes of a `sqlite.db` 
as hex into textbox, and it'll parse the fields out.~~

```python collapse={5-24, 30-49} {50}
@dataclass
class FileHeader:
    magic_bytes: bytes
    page_size: int
    format_write_version: int
    format_read_version: int
    reserved_bytes: int
    max_embedded_payload_frac: int
    min_embedded_payload_frac: int
    leaf_payload_frac: int
    file_change_counter: int
    file_size_pages: int
    freelist_trunk_ptr: int
    num_freelist_pgs: int
    schema_cookie: int
    schema_format_number: int
    default_page_cache_size: int
    largest_root_b_tree_ptr: int
    text_encoding: str
    user_version: int
    vacuum_mode: bool
    app_id: int
    version_valid_for: int
    sqlite_version_number: int

    def __init__(self, file: BinaryIO):
        (
            self.magic_bytes,
            self.page_size,
            self.format_write_version,
            self.format_read_version,
            self.reserved_bytes,
            self.max_embedded_payload_frac,
            self.min_embedded_payload_frac,
            self.leaf_payload_frac,
            self.file_change_counter,
            self.file_size_pages,
            self.freelist_trunk_ptr,
            self.num_freelist_pgs,
            self.schema_cookie,
            self.schema_format_number,
            self.default_page_cache_size,
            self.largest_root_b_tree_ptr,
            text_encoding,
            self.user_version,
            vacuum_mode,
            self.app_id,
            self.version_valid_for,
            self.sqlite_version_number,
        ) = struct.unpack(">16sH6B12i20x2i", file.read(100))
        self.text_encoding = {
            1: 'utf8',
            2: 'utf16-le',
            3: 'utf16-be'
        }[text_encoding]
        self.vacuum_mode = vacuum_mode != 0
```

Most of the heavy lifting here is happening on line 50. I'm using the python `struct.unpack` module
to parse the first 100 bytes of the file and break it up into a list of separate values that can be
manipulated easily. The format string I'm passing (`>16sH6B12i20x2i`) can be read like this:

| code | meaning                                                        |
|------|----------------------------------------------------------------|
| >    | Use little endian                                              |
| 16s  | An array of 16 bytes of raw data                               |
| H    | A single unsigned short (2 byte) value                         |
| 6B   | 6 unsigned 1-byte integers                                     |
| 12i  | 12 signed 4-byte integers                                      |
| 20x  | A gap of 20 bytes (reserved in the sqlite spec for future use) |
| 2i   | 2 integer values                                               |

The only field of importance right now is the `pageSize` field, but I parsed everything out to make life
a little simpler later. Pages are the highest level structure
in a sqlite file, so knowing how big they are is pretty useful. We'll skip the rest 
of the fields for now, but some of them will be coming up a little later.

## The `sqlite_schema` table
Now that we know the page size, we need to parse the special `sqlite_schema` table. This is a
special table that gives us information about the other tables in the database. The first page
of the database file is always the root of the b-tree for this table, so it's very easy to find.

Conveniently for us, the example database I set up has only 1 table and a few indexes, so the entire
`sqlite_schema` table fits in one page. That means that the root page (i.e. page 1) is also a leaf
page, which means (for now) we only need to implement reading leaf table pages!

The first step is to parse the header for the page. I used an extremely similar pattern to what I did
to parse the file level header:

```python {11}
class BTreePageType(Enum):
    TABLE_INTERIOR = 0x05
    TABLE_LEAF = 0x0D
    INDEX_INTERIOR = 0x02
    INDEX_LEAF = 0x0A

@dataclass
class PageHeader:
    type_id: BTreePageType
    first_free_block_offset: int
    num_cells_in_page: int
    cell_content_area_offset: int
    fragmented_free_bytes: int
    right_ptr: Optional[int]
    header_size: int

    def __init__(self, file: BinaryIO):
        self.type_id = BTreePageType(file.read(1)[0])
        (self.first_free_block_offset,
         self.num_cells_in_page,
         self.cell_content_area_offset,
         self.fragmented_free_bytes) = struct.unpack(">3HB", file.read(7))
        if self.type_id in (BTreePageType.TABLE_INTERIOR, BTreePageType.INDEX_INTERIOR):
            self.right_ptr = struct.unpack(">I", file.read(4))[0]
            self.header_size = 12
        else:
            self.right_ptr = None
            self.header_size = 8
```

You may have noticed in the diagram earlier that there's one pointer at the end of the internal page
that's not part of a cell:

This special pointer is used when we're searching for a value that's greater than all the keys in the list.
So in that example, if we were looking for a key of `1000`, we'd exhaust all the keys in our internal page,
and then follow that right-most pointer to get to the next page. Since that last pointer doesn't fit in any
of the cells, it's stored in the header (line 13 in the above code snippet). This is only relevant for internal
pages though, so we only set that `rightPtr` field on the relevant page types.

In my example database, the page header for the `sqlite_schema` table looks like this:

| Field                    | Value      |
|--------------------------|------------|
| type_id                  | TABLE_LEAF |
| first_free_block_offset  | 4088       |
| num_cells_in_page        | 4          |
| cell_content_area_offset | 3680       |
| fragmented_free_bytes    | 0          |
| right_ptr                | None       |
| header_size              | 8          |

As expected, this is a `TABLE_LEAF` page. For reading data, we don't really have to care about free blocks or fragmented
free bytes, so we'll ignore those fields.

In a table leaf page like this one, each cell corresponds to one row in the table. So `num_cells_in_page` tells us that this
table has 4 items in it. Reassuringly, this lines up with what we get from reading the table using the sqlite CLI:

```sql frame="terminal" showLineNumbers=false
sqlite> SELECT COUNT(1) FROM sqlite_schema;
4
```

Incidentally, this explains why `SELECT COUNT(1) FROM some_big_table` queries are so slow! The database engine would need
to walk the b-tree to find every leaf pages in the table, and add up the `num_cells_in_page` for every one to get the
exact answer.

Anyway, we now have everything we need to read the cells from the table! Here's how I implemented that:

```python {10-16}
@dataclass
class BTreePage_TableLeaf:
    offset: int
    pageHeader: PageHeader
    cells: List[TableLeafCell]

    def __init__(self, file: BinaryIO):
        self.offset = file.tell()
        self.cells = []
        self.pageHeader = PageHeader(file)

        num_cells = self.pageHeader.numCellsInPage
        cell_ptrs = struct.unpack(f'>{num_cells}H', file.read(num_cells * 2))
        for ptr in cell_ptrs:
            file.seek(self.offset + ptr)
            self.cells.append(TableLeafCell(file, database.header.text_encoding))
```

This is pretty strait forward. First we read the page header to figure out how many cells
to expect. Immediately after the header ends there's an array of offsets (relative to the
beginning of the page) which tell us where to find the cells. We iterate over those offsets
and parse each cell! Each cell contains its key, and array of values, one value per column 
in the table! Later on we'll dig into what actually happens to parse a cell into those values,
but it's really just an implementation detail.

Here's what we get by dumping all the cell payloads in the sqlite_schema table:


```
['table', 'users', 'users', 2, 'CREATE TABLE users (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    username TEXT NOT NULL UNIQUE,\n    email TEXT NOT NULL UNIQUE,\n    password_hash TEXT NOT NULL,\n    created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n)']
['index', 'sqlite_autoindex_users_1', 'users', 3, None]
['index', 'sqlite_autoindex_users_2', 'users', 4, None]
['table', 'sqlite_sequence', 'sqlite_sequence', 5, 'CREATE TABLE sqlite_sequence(name,seq)']
```

This is identical (albeit not quite so nicely formatted) to the output of selecting the rows of the table:
```sql frame="terminal" showLineNumbers=false
sqlite> SELECT * FROM sqlite_schema;

table|users|users|2|CREATE TABLE users (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username TEXT NOT NULL UNIQUE,
    email TEXT NOT NULL UNIQUE,
    password_hash TEXT NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
)
index|sqlite_autoindex_users_1|users|3|
index|sqlite_autoindex_users_2|users|4|
table|sqlite_sequence|sqlite_sequence|5|CREATE TABLE sqlite_sequence(name,seq)
```

Paraphrasing the docs, the rows here each represent an object in the database. The columns represent (in order):

- The type of object (table/index/trigger/etc)
- The name of the object (in the case of a table, it's the name of the table)
- The table that the object references
	- For a table, this is just the table's name again
	- For an index, this is the table that the index is on
- **The root page number for the object**
- The sql used to create the object. This is empty for the implicit indexes that sqlite created as a result of 
the `UNIQUE` constraints on the `username` & `email` columns

Notably, for some reason there isn't a column in this table to tell us what column(s) an object refers to.
This turned out to be an issue later on when I tried to use indexes, since there isn't an easy way to figure
out which index covers a specific column. The only way to do it is to parse the SQL, which seems a bit silly.

Anyway, for now all we need is the root page number of the `users` table: 2.

## Reading the `users` table
Now that we know that the root of the `users` table is page `2`, we can repeat this process for that tree! Except
there's a hitch. Since I populated this table with 1000 records (as opposed to the 4 records in the sqlite_schema
table), this table doesn't fit on one page. So we've got slightly more work to do here to find the leaf page.

Since we're dealing with multiple page types now, this seems like a good time to
do a little refactoring. Specifically, I'm splitting things out into an abstract
base class called `Page`, which will help keep things organized as we have to deal
with more and more different types of database tables.

```python ins={1-25,37,49-54} del={38-41}
@dataclass
class Page(ABC):
    offset: int
    pageHeader: PageHeader
    cells: List[Cell]

    def __init__(self, file: BinaryIO):
        self.offset = file.tell()
        # If this is the first page of the database, the file-level header shifts everything by 100 bytes
        if self.offset <= 100:
            self.offset = 0
        self.cells = []
        self.pageHeader = PageHeader(file)

    @staticmethod
    def build_page(file: BinaryIO) -> "Page":
        page_type = BTreePageType(file.read(1)[0])
        file.seek(-1, os.SEEK_CUR)
        match page_type:
            case BTreePageType.TABLE_LEAF:
                return BTreePage_TableLeaf(file)
            case BTreePageType.TABLE_INTERIOR:
                return BTreePage_TableInterior(file)
            case _:
                raise ValueError(f"Unsupported page type: {page_type}")
		
		@abstractmethod
    def get_record(self, key: any) -> Optional["Record"]:
        pass


@dataclass
class BTreePage_TableLeaf(Page):
    cells: List[TableLeafCell]

    def __init__(self, file: BinaryIO):
        super().__init__(file)

				self.offset = file.tell()
        self.cells = []
        self.pageHeader = PageHeader(file)

        num_cells = self.pageHeader.num_cells_in_page
        cell_ptrs = struct.unpack(f'>{num_cells}H', file.read(num_cells * 2))
        for ptr in cell_ptrs:
            file.seek(self.offset + ptr)
            self.cells.append(TableLeafCell(file, db.header.text_encoding))
		
		def get_record(self, row_id: int) -> Optional[Record]:
			idx = bisect_left(self.cells, row_id, key=lambda cell: cell.row_id)
			if idx < len(self.cells) and self.cells[idx].row_id == row_id:
					return self.cells[idx].record
			else:
					return None
```

With that refactoring out of the way, it's pretty easy to add support for `TableInterior` pages:

```python
@dataclass
class BTreePage_TableInterior(Page):
    cells: List[TableInteriorCell]

    def __init__(self, file: BinaryIO):
        super().__init__(file)

        num_cells = self.pageHeader.num_cells_in_page
        cell_ptrs = struct.unpack(f'>{num_cells}H', file.read(num_cells * 2))
        for ptr in cell_ptrs:
            file.seek(self.offset + ptr)
            self.cells.append(TableInteriorCell(file))
		
		def get_record(self, row_id) -> Optional[Record]:
			idx = bisect_left(self.cells, row_id, key=lambda cell: cell.key)
			if idx < len(self.cells) and self.cells[idx].key >= row_id:
					child_page = self.database.get_page(self.cells[idx].child_page_ptr)
			else:
					child_page = self.database.get_page(self.pageHeader.right_ptr)
			return child_page.get_record(row_id)
```

The main difference here is that instead of `get_record` being able to return the
record directly, it instead searches for a child page and then delegates looking up the
data to the child. If the child is another `TableInterior` page then it'll keep delegating
down the tree recursively until we eventually hit a leaf that returns a value.


And that's all we need to read records out of the database! I added one more `Database` abstraction
to help keep things organized:
```python
class Database:
    def __init__(self, file: BinaryIO):
        self.file = file
        self.header = FileHeader(file)
        self.schema_page = self.get_page(1)

    def get_root_page_num(self, target_object_name, target_object_type="table"):
        for record in self.schema_page.records:
            object_type, object_name, object_table, object_page, schema = record.values
            if object_name == target_object_name and object_type == target_object_type:
                return object_page

    def get_page(self, page_number) -> Page:
        page_offset = (page_number - 1) * self.header.page_size
        if page_offset == 0:
            page_offset += 100
        self.file.seek(page_offset)
        return Page.build_page(self, self.file)
```

and now we're ready to go! Looking back at our original goal, we wanted to look up the data for the
user with row id `450`. Here's how we can do that:

```python
with open("./example.db", "rb") as file:
		db = Databaes(file)
		users_table_page_num = db.get_root_page_num("users")
		users_table_root = db.get_page(users_table_page_num)
		record = users_table_root.get_record(row_id)
		print(record.values)
```

## Addendum: Parsing records
I glossed over one important part above, which is the actual format that the data is stored inside
the payload of each cell in the database. I chose to do that because some details of the format of 
a cell depend on the type of page we're in.